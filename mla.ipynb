{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ec79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))  # 可学习参数γ\n",
    "\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        # 计算平方均值的根 (RMS)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.gamma * self._norm(x.float()).type_as(x)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    hidden_size: int\n",
    "    num_heads: int\n",
    "    max_seq_len: int\n",
    "    rope_theta: float\n",
    "\n",
    "    attention_dropout: float\n",
    "\n",
    "    q_lora_rank: int\n",
    "    qk_rope_head_dim: int\n",
    "    kv_lora_rank: int\n",
    "\n",
    "    v_head_dim: int\n",
    "    qk_nope_head_dim: int\n",
    "    attention_bias: bool\n",
    "\n",
    "\n",
    "class MLA(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "        self.rope_theta = config.rope_theta\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "\n",
    "        self.q_lora_rank = config.q_lora_rank\n",
    "        self.qk_rope_head_dim = config.qk_rope_head_dim\n",
    "        self.kv_lora_rank = config.kv_lora_rank\n",
    "\n",
    "        self.v_head_dim = config.v_head_dim\n",
    "        self.qk_nope_head_dim = config.qk_nope_head_dim\n",
    "        self.attention_bias = config.attention_bias\n",
    "\n",
    "        self.out_proj = nn.Linear(\n",
    "            self.num_heads * self.v_head_dim, self.hidden_size, bias=False\n",
    "        )\n",
    "        # MLA压缩部分，一般是从7168 -> 1536，压缩比是 4.67\n",
    "        self.q_down_proj = nn.Linear(\n",
    "            self.hidden_size, self.q_lora_rank, bias=config.attention_bias\n",
    "        )\n",
    "        self.q_down_norm = RMSNorm(self.q_lora_rank)\n",
    "\n",
    "        self.kv_down_proj = nn.Linear(\n",
    "            self.hidden_size,\n",
    "            self.kv_lora_rank + self.qk_rope_head_dim,\n",
    "            bias=config.attention_bias,\n",
    "        )\n",
    "        self.kv_down_norm = RMSNorm(self.kv_lora_rank)\n",
    "\n",
    "\n",
    "        # 升维部分\n",
    "        self.q_head_dim = self.qk_nope_head_dim + self.qk_rope_head_dim\n",
    "        self.q_up_proj = nn.Linear(\n",
    "            self.q_lora_rank,\n",
    "            self.num_heads * self.q_head_dim,\n",
    "            bias=config.attention_bias,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
